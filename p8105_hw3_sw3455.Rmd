---
title: "p8105_hw3_sw3455"
author: "Shiying Wu"
date: "2024-10-13"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

### Problem 1
```{r clean_data}
data("ny_noaa")
ny_noaa <- ny_noaa |> 
  janitor::clean_names() |>
  separate(date, into = c("year", "month", "day"), convert = TRUE) |> 
  mutate(tmax = as.numeric(tmax) / 10,  
         tmin = as.numeric(tmin) / 10,
         prcp = prcp / 10, 
         snow = snow / 10 
        )

ny_noaa |>
  filter(!is.na(snow)) |>
  count(snow) |>
  arrange(desc(n))

```


This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 

id: Weather station identifier.
date: Date of the observation.
prcp: Precipitation (tenths of mm).
snow: Snowfall (mm).
snwd: Snow depth (mm).
tmax: Maximum daily temperature (tenths of degrees C).
tmin: Minimum daily temperature (tenths of degrees C).

I clean the dataset by creating separate variables for year, month, and day from the date column. I also convert tmax, tmin, precipitation, and snowfall to appropriate units (Celsius for temperatures and centimeters for snowfall). Upon examining snowfall, I find that 0 is the most commonly observed value, which makes sense as snowfall does not occur on most days in New York. The second most common value is NA, indicating missing data. Other frequently observed values include 13, 25, and 51, which suggest that snowfall was originally recorded in inches and later converted to millimeters.

```{r}
ny_noaa |> 
  filter(month %in% c(1, 7)) |> 
  group_by(id, year, month) |> 
  summarize(avg_tmax = mean(tmax, na.rm = TRUE)) |> 
  ggplot(aes(x = year, y = avg_tmax, group = id, color = factor(month))) + geom_point() + geom_path() +
  facet_grid(~month) +
  labs(title = "Mean monthly temperature for each station across years for January and July")
```
It generates a two-panel plot displaying the average maximum temperature for each station in January and July across years. As expected, the average temperature in January is significantly lower than in July for all stations and over all years. The stations exhibit similar temperature patterns, with synchronized peaks and troughs over time—when one station records a high average temperature for a given year, most other stations show similar behavior. However, there is one notably cold outlier in July around 1987 or 1988, along with a few other smaller deviations from the overall trend.

```{r}
hex = 
  ny_noaa |> 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  labs(title = "Hex Plot of tmin vs tmax", x = "Minimum Temperature (°C)", y = "Maximum Temperature (°C)")

ridge = 
  ny_noaa |> 
  filter(snow < 100, snow > 0) |>
  ggplot(aes(x = snow, y = as.factor(year))) + 
  geom_density_ridges() +
  labs(title = "Snowfall Distribution by Year", x = "Snowfall (cm)", y = "Year")

hex + ridge
```


This two-panel plot includes: (i) a hex plot of tmax versus tmin for the full dataset, and (ii) a ridge plot showing the distribution of snowfall values (in cm) greater than 0 and less than 100, broken down by year.

From the hex plot, I observe that while there is some variability in the data, most of the observations are tightly clustered around the center of the distribution. This suggests a strong correlation between minimum and maximum temperatures. Interestingly, there are a few rare cases where tmax is recorded as less than tmin, which points to potential issues with data recording or quality.

The ridge plot shows a multimodal distribution of snowfall within each year. Most stations report snowfall amounts between 0 and 4 cm. This pattern aligns with the earlier observation of common snowfall values in the dataset.

### Problem 2
```{r P2 data cleaning}
nhanes_accel <- read_csv("data/nhanes_accel.csv", 
                         na = c(".", "NA", "")) |>
                janitor::clean_names()

nhanes_covar <- read_csv("data/nhanes_covar.csv",
                         skip = 4,
                         na = c(".", "NA", "")) |> 
                janitor::clean_names()|>
                filter(age >= 21) |> 
                na.omit() |> 
                mutate(sex = factor(sex, levels = c("1", "2"), labels = c("Male", "Female")),
                       education = factor(education, levels = c("1", "2", "3"),
                       labels = c("Less than High School", "High School Equivalent", "More than High School")))

nhanes_merged <- left_join(nhanes_covar, nhanes_accel, by = "seqn")
```

For problem 2, I import data using `read_csv` and clean names using `janitor::clean_names()`. Then, I filtering out individuals under 21 and removing any rows with missing demographic data and  change categorical number into what it means. The final step, I used left join to merges demographic and accelerometer data, which all the one in nhanes_accel data that does not have demographic data will be removed. This dataset contains `r nrow(nhanes_merged)` rows and `r ncol(nhanes_merged)` columns. Variables include `r colnames(nhanes_merged[1:6])` and more.

```{r P2 education and sex table}
nhanes_merged |>
  group_by(education, sex) |>
  summarise(n_obs = n()) |>
  pivot_wider(
    names_from = sex,
    values_from = n_obs) |> 
  knitr::kable()
```

After I `group_by` education and sex,  I used summarise to calculated the number of observations for each group. Following this, I make the table wider by `pivot_wider` for more tidyness as sex to be column and education to be row. Finaly rendered into a reader-friendly table using `knitr::kable()`.

The table shows a similar number of females and males at both `Less than High School` and `More than High School`, the gender disparity is similar when we think about education both `Less than High School` and `More than High School`. In contrast, more males than females have a `High School Equivalent` education. It is a worth to study question that why such big difference appear only in high school equivalent education

```{r P2 Age distribution, fig.width = 8, fig.asp = .6}
nhanes_merged|>
  select(age, sex, education)|>
  ggplot(aes(x = age, fill = sex)) +
  geom_density(alpha = 0.4, adjust = 0.5) + 
  facet_wrap(~ education) +
  labs(title = "Age Distribution by Sex and Education", 
       x = "Age", 
       y = "Density") + 
  viridis::scale_fill_viridis(discrete = TRUE)
```

I used the nhanes_merged dataset to explore the age distribution by sex and education level by density plots. By selecting the variables that we looking for (age, sex, and education), and plotting age as a density function with color of sex, and put education in different panel for more easy to understand the difference.

Both genders show a peak in young age groups in `More than High School`, which lead to a higher proportion of younger individuals pursuing higher education. But the female also has higher density in young age groups.

For `High School Equivalent` education, the female and male are opposite to each other. When male are high in young age group, female are lower than male, but in old age group, the women has higher density.

The `Less than High School`, gender are similar to each other. age group amount 60 has a low density compare to other age group that close to it.


```{r P2 Total activity, fig.width = 8, fig.asp = .6}
nhanes_merged |>
  rowwise() |>
  mutate(total_activity = sum(c_across(starts_with("min")))) |>
  ggplot(aes(x = age, y = total_activity, color = sex)) +
  geom_point(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  facet_wrap(~ education) +
  labs(title = "Total Daily Activity vs Age by Sex and Education Level",
       x = "Age", 
       y = "Total Activity") + 
  viridis::scale_fill_viridis(discrete = TRUE) +
  theme(legend.position = "bottom")
```

In the analysis, I sum all activity start with "min" data, then use ggplot's `geom_point` and `geom_smooth` visualized this total activity against age by both point and lines, while use sex as color and put in different panels by education level. 

Activity peaks in mid-life across all educational backgrounds, suggesting a higher engagement in physical activity among middle-aged adults. Males generally showing slightly higher activity levels in less than high school education while men are older then 40, in other education and age all showes women has higher total activity. The trend lines, shows a  overall decrease in activity as age increases. 


```{r P2 Activity time, fig.width = 8, fig.asp = 0.6}
nhanes_merged |>
  pivot_longer(cols = starts_with("min"), names_to = "minute", values_to = "activity") |>
  mutate(minute = as.numeric(str_extract(minute, "\\d+"))) |>
  ggplot(aes(x = minute, y = activity, color = sex)) +
  geom_line(size = 0.1, alpha =  0.5) +
  geom_smooth(se = FALSE, size = 1) +
  facet_wrap(~ education) +
  labs(title = "24-hour Activity Time Courses by Education Level and Sex", y = "Activity", x = "Minutes of 24 Hours") + 
  viridis::scale_fill_viridis(discrete = TRUE) +
  theme(legend.position = "none")
```

I transformed data into a long format to easier analyze 24-hour activity patterns by using pivot_longer. The plot is faceted by education level, and lines represent activity over time for males and females. x for minute and y for activity in every minuts to show the overall activity for each data.

The 24-hour activity patterns by education level and sex, showing a similarity of activity peaking midday and low at begining of the day which is midnight across all groups. The differences between males and females are small, female has slightly higher activity peaks in all education group. More than High School education, female seems has bigger difference with male in midday than other two education group. Individuals with less than high school education tend to have higher peaks around 700 minuts which is around 11:30 AM. 


### Problem 3
```{r P3 data prepareing}
Jan2020Citi <- read_csv("data/citibike/Jan 2020 Citi.csv", 
                         na = c(".", "NA", "")) |>
                janitor::clean_names() |>
                mutate(year = 2020,
                       month = "Jan") 

Jan2024Citi <- read_csv("data/citibike/Jan 2024 Citi.csv", 
                         na = c(".", "NA", "")) |>
                janitor::clean_names()|>
                mutate(year = 2024,
                       month = "Jan") 

July2020Citi <- read_csv("data/citibike/July 2020 Citi.csv", 
                         na = c(".", "NA", "")) |>
                janitor::clean_names()|>
                mutate(year = 2020,
                       month = "July") 

July2024Citi <- read_csv("data/citibike/July 2024 Citi.csv", 
                         na = c(".", "NA", "")) |>
                janitor::clean_names()|>
                mutate(year = 2024,
                       month = "July") 

Citi_df <- bind_rows(Jan2020Citi, Jan2024Citi, July2020Citi, July2024Citi) |>
  relocate(year, month, weekdays)
```
I read each csv file independently and clean their name and mutate the year and month variable in order for us to know when is the dataset come from. Then use bind rows to combine all four dataset together. The final dataset, Citi_df, contains  ride duration, start and end times, station ID, rideable_type, and user type (member or casual user). I placing the most relevant time variables (year, month) first, for tidyness.This dataset contains `r nrow(Citi_df)` rows and `r ncol(Citi_df)` columns.  Variables include `r colnames(Citi_df)`.

```{r P3 member_casual table}
Citi_df |>
  mutate(date = paste(year, month, sep=" ")) |>
  group_by(date, member_casual) |>
  summarise(number_of_rides = n()) |> 
  pivot_wider(
    names_from = member_casual, 
    values_from = number_of_rides) |>
  knitr::kable()
```
I put the year and month together as date for more reader friendly table, then I group by date and rider type and in order for total number of it. Eventually, I pivot wider to make rider type as column and date as row and use knitr::kable() clean table.

There are several difference, the member has more rides than casual. Also there are more rides in july than Jan may due to it is warmer and also 2024 clearly has more rides than 2020.


```{r P3 top 5 start station}
Citi_df |>
  filter(year == 2024, month == "July") |>
  group_by(start_station_name) |>
  summarise(number_of_rides = n()) |>
  arrange(desc(number_of_rides)) |>
  head(n = 5) |>
  knitr::kable()
```
I first filter 2024 july dataset, then group data by start_station_name and summarise how many rides happen in each start station, and by arrange it from more to less, first 5 would be the top 5 popular start station we looking for.

```{r P3 Median Ride Duration, fig.width = 8, fig.asp = .6}
Citi_df |>
  group_by(year, month, weekdays) |>
  summarise(median_ride_duration = median(duration)) |>
  ggplot(aes(x = weekdays, y = median_ride_duration, group = interaction(year, month), color = month)) +
  geom_line() +
  facet_wrap(~year) +
  scale_x_discrete(limits = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")) +
  labs(title = "Median Ride Duration Across Different Times") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  
```

I group by year month and weekdays in order to get median_ride_duration for each times. then use month as color, weekdays as x and seperate as two panels by year. On the graph, we can clearly see that 2020 has significant larger median ride duration than 2024 overall. For 2020 jan , july and 2024 july, the weekend  has higher median ride duration than work days, but for 2024 jan, it is more stable. Overall, as monday to sunday, the overall trend are increasing. 

```{r P3 Impact on Duration, fig.width = 8, fig.asp = .8}
Citi_df |>
  filter(year == 2024) |>
  ggplot(aes(x = duration, fill = member_casual)) +
  geom_histogram(position = "identity", alpha = 0.7) +
  facet_wrap(month~rideable_type) +
  labs(title = "Impact on Ride Duration in 2024")
```

I filtered the 2024 Citi Bike data to focus on ride durations, categorized by membership status (member & casual) and bike type (classic & electric). Using ggplot, I created histograms to visually compare the distributions of ride durations within these categories. The histograms are displayed separately for each bike type through the use of facet_wrap, highlighting differences in usage patterns between classic and electric bikes. Members in general show higher duration than casual overall. For Electric bikes, both member and casual riders of electric bikes tend to have longer ride durations compared to those using classic bikes. July has longer duration than Jane overall. Both distributioon are really skewness. Also the majority of duration is with 50, a lot of people took it as a short trip.







